{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is an [jupyter](http://jupyter.org) notebook.\n",
    "Lectures about Python, useful both for beginners and experts, can be found at http://scipy-lectures.github.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the notebook by (1) copying this file into a directory, (2) in that directory typing \n",
    "jupyter-notebook\n",
    "and (3) selecting the notebook.\n",
    "\n",
    "***\n",
    "Written By: **Shashwat Shukla, Om Kolhe**\n",
    "***\n",
    "\n",
    "In this exercise, we will learn and code about Adaboost algorithm and look at one of it's application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the details of the Adaboost algorithm, a brief overview of what Adaboost is and what it has to do with Machine Learning will prove useful to the uninitiated.\n",
    "\n",
    "First of all, what are we trying to achieve here? \n",
    "Basically, we are given a dataset. For each datapoint, we have measured and stored the value of some parameters of interest. \n",
    "We have also assigned each datapoint a **label**. That is, the dataset that we have is already classified. It could have been classified by a human or the label could be assigned on the basis of experimental/empirical observation.\n",
    "\n",
    "It's best explained by an example. So let's say we conducted a census in India. We choose our dataset to be a subset of all the census data that we have collected. We manually label all the data in our subset. Let's say that our subset contained a million people. Given that there are a billion people in India, we don't want to do the rest of the labelling by hand! This is where Adaboost comes in. We will used the data we have already labelled as **training data** and feed it into the Adaboost algorithm. Adaboost 'learns' from this data. Now if you feed it the rest of the census data, which was not labelled, it will generate those labels for you. \n",
    "\n",
    "In the census example, each datapoint actually represents a person. The parameters of interest include height, weight, income, assets owned, marital status etc. \n",
    "Here classification/labelling can mean anything: people can be classified into being above or below the poverty line. A more complex classification can be whether or not a person is eligible for a loan of (say) Rs. 10lakhs from a bank; more generally,  whether a person is credit worthy.\n",
    "\n",
    "Note that both examples are **binary classifications**. Meaning that the labels can only be one of two values, like a simple yes or no. This is important because Adaboost assumes, and only works on binary classsified data.\n",
    "\n",
    "\n",
    "\n",
    "Alright, so Adaboost stands for **Adaptive Boosting**. So what is it boosting, and why is it called adaptive?\n",
    "Let's go back to the census example and consider again the question of classifying the entire population of India as being eligible for a bank loan of Rs. 10 lakhs ie credit worthiness. I shall refer to this problem as credit worthiness henceforth.\n",
    "\n",
    "Recall some of the parameters that we record for each person: height, weight, income, assets owned, marital status.  \n",
    "Let's pick one parameter, say income. How would you generate a classifier for our dataset using only this parameter? \n",
    "That's easy: just pick a **threshold** say, an income of Rs 5 lakhs a year. This classifier will label any person with an income greater than 5 lakhs as being credit worthy and other people people as non credit worthy. \n",
    "We could have generated a different classifier by simply choosing a different threshold. Like an income of 10 lakhs a year.\n",
    "So here's the **important point:** a classifier can be uniquely generated by choosing a parameter, and a threshold for that parameter.\n",
    "\n",
    "Back in the census example, you must have realised that some parameters are more important than others: height and weight don't really have much to do with credit worthiness. Income and assets owned are definitely very important. In the real world, banks sometimes also factor in marital status(as a sign of being responsible). Now the obvious question is, how do we quantitatively combine income, assets, marital status etc. How much weightage should income, assets, marital status get in determining the final labels? What thresholds do we choose for these paramters?? \n",
    "All of these questions can be answered in one go: Ask the data! We have already classified some of the data. The magic of Adaboost is that it extracts all of this information about weights and thresholds for us.\n",
    "This is the **\"boosting\"** in Adaboost. It boosts the weightage of classifiers that are more relevant. \n",
    "\n",
    "We need to define another term: **weak classifiers**. A weak classifier is just a classifier that is better than pure guessing. That is, it classifies more than half of the labelled data correctly. Even if it's success rate is just 51 % or even 50.01 %,, it still counts as a weak classifier. \n",
    "A **strong classifier** is one that has a very good success rate, like 99% or whatever.\n",
    "(The astute reader would point out that every strong classifier is also a weak classifier. That's really not the point. These are just heuristic definitiions used to convey obvious differences in success rates.)\n",
    "\n",
    "Now we are in a position to understand the Adaboost algorithm:\n",
    "\n",
    "1) Adaboost is iterative: In the first iteration, it picks the most accurate weak classifier. In the census example, it would most probably correspond to the parameter of income, with some threshold value(Note: classifiers that work with income as a parameter but have different threshold values are distinct classifiers).\n",
    "\n",
    "2) So we have picked our first weak classifier. Now what? Well, this weak classifier must have obviously misclassified a lot of datapoints(a little less than half). The classifier that we pick in the next iteration should then be better at correctly classifying this misclassified data. How do we choose such a classifier? The way it works in Adaboost is that **each data point is assigned a weight**. In the first iteration, obviously all the datapoints had equal weightage. Then the weight of the misclassified datapoints is incremented, and that of correctly labelled data is decremented. So the error rate of the second classifier is calculated as a weighted sum: if it misclassifies an already misclassified datapoint, it gets a higher error rate. \n",
    "\n",
    "3) In the second iteration, the classifier with the smallest error rate on the weighted data is chosen. Now we have chosen two weak classifiers. We again increment the weights of the datapoints that are collectively misclassified by the two weak classifiers, and decrement the weigths of the ones that they correctly classify.\n",
    "\n",
    "4) Repeat this process of reassigning weights and picking weak classifiers on the weighted data until you have made the error rate as small as you like. Here we are referring to the error rate of the final strong classifier( what we form out of all the weak classifiers that we have picked so far).\n",
    "\n",
    "So in a nutshell: Adaboost takes a lot of weak classifiers, assigns them appropriate weights, to make a strong classifier.\n",
    "\n",
    "Okay. So far, we haven't encountered a single mathematical equation! Now that we understand *what* Adaboost does, we need to look at *how* it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to understand the math is to see it in action. Here we will code the adaboost algorithm and use it to classify some data.\n",
    "\n",
    "First, we import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise some variables that we will need:\n",
    "-> N: The number of samples or data-points.\n",
    "-> T: The number of iterations in our boosting algorithm.\n",
    "-> dim: The number of parameters recorded for each data-point.\n",
    "\n",
    "Unlike in the census example, we haven't actually collected any data or labelled it. So we will generate our own sample data and label it in a simple manner:\n",
    "-> x: The data. It is an N x dim matrix.\n",
    "-> label: N x 1 array that stores the known labels for each data-point. Here label belongs to {-1, 1}\n",
    "\n",
    "Then we plot the original data. This is the learning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 20  \n",
    "dim = 2\n",
    "N = 1000\n",
    "\n",
    "x = np.random.randn(N, 2)  # dim=2\n",
    "\n",
    "label = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "# label = x[:,0] < x[:,1]  #linear separation example\n",
    "label = (x[:, 0]**2 + x[:, 1]**2) < 1  # nonlinear separation example\n",
    "\n",
    "\n",
    "label = label * 1.0\n",
    "\n",
    "pos1 = np.nonzero(label == 1)\n",
    "pos2 = np.where(label == 0)\n",
    "label[pos2] = -1\n",
    "\n",
    "# Plot the data\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[pos1, 0], x[pos1, 1], 'b*')\n",
    "plt.plot(x[pos2, 0], x[pos2, 1], 'r*')\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.title(\"Original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed how a classifier can be defined by selecting a parameter and a threshold for this parameter. \n",
    "The output of this classifier that we have defined would be +1 or -1 depending on whether the input's value for this parameter is greater than or smaller than the threshold, as we have discussed before.\n",
    "\n",
    "But we had also talked about generating an error rate for weak classifiers in every iteration, on the weighted dataset. \n",
    "The following lines of code define a function weak_Classifier_error() that takes a weak classifier, a labelled and weighted dataset as input. \n",
    "*It's output is the error rate for this weak classifier on the dataset for the given weights.*\n",
    "And the error rate, as mentioned before, just a weighted sum of the errors. \n",
    "The weigths are: 0 if it is correctly classified and 1 if incorrectly classified.\n",
    "This simplfies to simply:\n",
    "**Error rate= Sum of weigths of datapoints that were classified incorrectly**\n",
    "\n",
    "But wait you say: The arguments for this function are threshold, dimension, sign, weight and label. Where is the weak classifier? Remember, how we said that a classifier is *uniquely* defined by it's parameter and threshold values? So we just input these.\n",
    "\n",
    "Also, the use of 'sign' is simple: It basically flips the classifier on it's head. If sign is +1, then it will classify data as +1 if it is *greater* than threshold. If sign is -1, it will classify data as +1 if it is *smaller* than threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "\n",
    "# Returns error and calculated labels corresponding to\n",
    "def weakClassifier_error(i, j, k, x, weight, label):\n",
    "                                                # threshold i\n",
    "                                                # dimension j\n",
    "                                                # sign k on dataset x.\n",
    "                                                # Original labels are stored in\n",
    "                                                # label\n",
    "\n",
    "    temp_err = np.float64(0)\n",
    "    # Initialise actual and expected labels to a perfect match( 0 = match , 1\n",
    "    # = not a match)\n",
    "    y = np.zeros(N, dtype=np.int64)\n",
    "\n",
    "    if(k == 1):\n",
    "        temp = (x[:, j] >= i)\n",
    "    else:\n",
    "        temp = (x[:, j] < i)\n",
    "\n",
    "    temp = np.int64(temp)\n",
    "    temp[np.where(temp == 0)] = -1\n",
    "    y = np.int64(temp != label)\n",
    "    # Calculate error of this weak classifier on the weighted dataset\n",
    "    temp_err = np.sum(y * weight)\n",
    "\n",
    "    return [temp_err, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally take a look at the actual code for the Adaboost algorithm. \n",
    "\n",
    "We define some variables that we need: \n",
    "-> h: Tx3 array that stores the weak classifiers selected after each iteration:\n",
    "       h[index][0]= threshold\n",
    "       h[index][1]= dim (data dimension)\n",
    "       h[index][2]= pos (the sign of the classifier, +1/-1)\n",
    "-> alpha: T x 1 array that stores the weight of each weak classifier chosen to\n",
    "            make up the final classifier.\n",
    "            \n",
    "The point of 'h' is obvious. However, 'alpha' needs an explanation:\n",
    "We said that the final classifier will be a combination of all the weak classifiers that we have selected over the iterations.\n",
    "But what does *combination* mean exactly? \n",
    "**In Adaboost, the final classifier is a weighted sum of the weak classifiers that we have selected.**\n",
    "\n",
    "*Now don't get confused*: we have talked about weights before. We assigned weights to each datapoint, over each iteration, so that we could select a new classifier. These weigths are stored in weights[].\n",
    "And the error rate was a weighted sum of the weights.\n",
    "\n",
    "What's beautiful about Adaboost is that it generates the *final* output in an analogous manner. \n",
    "The weigths we are talking about here are assigned to the weak classifiers themselves. \n",
    "These weights are stored in alpha[].\n",
    "The final output is the weighted sum of the outputs of each weak classifier.\n",
    "\n",
    "Alright: so now we have two sets of weights: One set for the datapoints and one set for the weak classifiers themselves. \n",
    "How are these weights determined?\n",
    "We said that the weigths corresponding to the datapoints were incremented or decremented based on whether or not they were correctly classified,  by the best weak classifier of that iteration.\n",
    "How is this done mathematically?\n",
    "\n",
    "First we calculate alpha[t] (refer to the code). Then we use it to reassign weights. \n",
    "The expression for alpha[t] isn't very revealing at first glance. But it is what it is, and it works. We will try and take a look at it in more detail later.\n",
    "\n",
    "The observant reader will immediately point out and maybe puzzled by the fact that just a few sentences ago I mentioned that the weights for the weak classifiers are stored in alpha[]. \n",
    "So is this is a different alpha? *No it's not!*\n",
    "This is an especially fascinating aspect of Adaboost. The weights for the weak classifiers and the weigths for the datapoints are linked by alpha[].\n",
    "\n",
    "The last line in the following code block renormalises the weights. This is necessary as we are reassigning weigths after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actual program begins\n",
    "\n",
    "# h and alpha together completely specify the final strong classifier\n",
    "h = np.zeros([T, 3], dtype=np.float64)\n",
    "alpha = np.zeros(T, dtype=np.float64)\n",
    "\n",
    "threshold = np.arange(-3.0, 3.0, 0.1)\n",
    "\n",
    "weight = np.ones(N, dtype=np.float64) / (N)  # Initialise weights\n",
    "\n",
    "# Initially set error to infinity, to allow comparing with error of classifiers\n",
    "err = np.ones(T, dtype=np.float64) * np.inf\n",
    "\n",
    "for t in range(T):\n",
    "    for i in threshold:\n",
    "        for j in range(dim):\n",
    "            for k in [-1, 1]:\n",
    "                [tmpe, y] = weakClassifier_error(i, j, k, x, weight, label)\n",
    "                if(tmpe < err[t]):  # storing the better classifier in h\n",
    "                    err[t] = tmpe\n",
    "                    y0 = y\n",
    "                    h[t][0] = i\n",
    "                    h[t][1] = j\n",
    "                    h[t][2] = k\n",
    "\n",
    "    if(err[t] > 0.5):\n",
    "        T = t\n",
    "        # We have run out of weak classifiers! So truncate the no: of\n",
    "        # iterations used\n",
    "        print t, \"Error!\"\n",
    "        break\n",
    "\n",
    "    alpha[t] = 0.5 * np.log((1.0 - err[t]) / err[t])\n",
    "\n",
    "    # y0=0 corresponded to correctly labelled datapoints. To reassign weights,\n",
    "    y0[np.where(y0 == 0)] = -1\n",
    "    # we need -1 and not 0 at these positions\n",
    "\n",
    "    weight = np.float64(weight * np.exp(alpha[t] * y0))  # Reassign weights\n",
    "\n",
    "    weight = weight / np.sum(weight)  # Normalise reassigned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished selecting all the weak classifiers and their corresponding weights. This is all that we need to generate the final output. \n",
    "In our code, we are not feeding it a new dataset. Instead, we are inputting the training dataset itself. This is an excellent way to see if our algorithm actually works. Because we can compare the original labels directly with the generated lables.\n",
    "\n",
    "ALright, so we now proceed to generate the final output. We said that the final output is the weighted sum of the outputs of all the weak classifiers. The weights, as we know, are stored in alpha[]\n",
    "\n",
    "We then proceed to plot the generated labels also.\n",
    "We also plot 'misshits', which is a measure of how the accuracy of our final classifier improves when we add the weak classifiers one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_sum = np.zeros(N, dtype=np.float64)\n",
    "temp = np.zeros(N, dtype=np.float64)\n",
    "final_label = np.zeros(N, dtype=np.float64)\n",
    "misshits = np.zeros(T)\n",
    "\n",
    "for t in range(T):  # Calculate final labels\n",
    "    temp = h[t][2] * np.sign(x[:, h[t][1]] - h[t][0])\n",
    "    temp_sum = np.float64(temp_sum + alpha[t] * temp)\n",
    "    final_label = np.sign(temp_sum)\n",
    "    misshits[t] = np.sum(np.float64(final_label != label)) / N\n",
    "\n",
    "\n",
    "# Now plot the generated labels\n",
    "pos1 = np.where(final_label == 1)\n",
    "pos2 = np.where(final_label == -1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[pos1, 0], x[pos1, 1], 'b*')\n",
    "plt.plot(x[pos2, 0], x[pos2, 1], 'r*')\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "plt.title(\"Generated data\")\n",
    "plt.show()\n",
    "\n",
    "# Plot miss hits when more and more weak learners are used\n",
    "plt.figure()\n",
    "plt.plot(misshits)\n",
    "plt.ylabel('Miss hists')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all of it.\n",
    "We have described all the mathematical details of Adaboost through our code. But with math comes proofs, and we haven't actually proven the convergence of the Adaboost algorithm. Neither have we derived the expression for alpha[t]. \n",
    "\n",
    "Proving convergence is pretty difficult. But you must know that the adaboost algorithm is guaranteed to converge to a strong classifier with an arbitrarily good success rate, given enough iterations(and some other caveats).\n",
    "\n",
    "For some intuition into where that weird expression of alpha came from, refer to [this](http://research.microsoft.com/en-us/um/cambridge/events/aistats2003/proceedings/156.pdf) link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
